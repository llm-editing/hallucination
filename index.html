<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LLMs Meet Authorship Attribution">
  <meta name="keywords" content="Authorship Attribution, LLM, Large Language Model, Computational Linguistic">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="Test for HalluEdit" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://github.com/llm-authorship" />
<meta property="og:url" content="https://github.com/llm-authorship/" />
<meta property="og:site_name" content="Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://github.com/llm-authorship/static/images/framework.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges" />
<meta name="twitter:description" content="Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges" />
<meta name="twitter:site" content="@BaixHuang" />
<meta name="twitter:image" content="https://github.com/llm-authorship/static/images/framework.png" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges","name":"Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges","url":"https://github.com/llm-authorship/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image Carousel</title>

<style>
  .carousel-container {
    position: relative;
    max-width: 800px;
    margin: auto;
  }

  .carousel-slide img {
    width: 100%;
    height: auto;
    display: block;
  }

  .caption {
    text-align: center;
    padding: 5px;
    background-color: #ddd;
  }

  .prev, .next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    font-size: 24px;
    color: black;
    background-color: rgba(255, 255, 255, 0.7);
    border: none;
    padding: 10px;
    border-radius: 0 3px 3px 0;
  }

  .next {
    right: 0;
    border-radius: 3px 0 0 3px;
  }
</style>

<style>
  .author-block, .institution-block {
    position: relative;
    display: inline-block;
  }

  .author-block sup, .institution-block sup {
    font-size: smaller;
    top: -0.6em;
  }

  .publication-authors a, .publication-authors span {
    margin-right: 5px;
  }

  .dot {
    height: 15px;
    width: 15px;
    margin: 0 2px;
    background-color: #6e4949;
    border-radius: 50%;
    display: inline-block;
    transition: background-color 0.6s ease;
  }

  .active, .dot:hover {
    background-color: #717171;
  }

  .carousel-dots {
    text-align: center;
  }

</style>

</head>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Horizontal Navigation Bar</title>
  <style>
      ul {
          list-style-type: none;
          margin: 0;
          padding: 0;
          overflow: hidden;
          background-color: #333;
      }

      li {
          float: left;
      }

      li a {
          display: block;
          color: rgb(255, 255, 255);
          text-align: center;
          padding: 14px 16px;
          text-decoration: none;
      }

      li a:hover {
          background-color: #111;
      }
  </style>
</head>

<ul>
  <!-- <li><a href="#home">Home</a></li> -->
  <li><a href="#Authorship-Attribution-in-the-Era-of-LLMs">Survey</a></li>
  <li><a href="#Benchmarks">Benchmarks</a></li>
  <li><a href="#Detectors">Detectors</a></li>
  <li><a href="#canllm-identify-authorship">Our Related Work (EMNLP)</a></li>
  <li><a href="#BibTeX">BibTex</a></li>
  <li><a href="#Contact">Contact</a></li>
</ul>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="content has-text-centered">
                  EMNLP 2024 Findings: 
                  <a href="#canllm-identify-authorship">Can Large Language Models Identify Authorship?</a>
                  <br>
                  AI Magazine 2024: 
                  <a href="https://llm-misinformation.github.io/" target="_blank">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>
                </div>
              </div>
            </div>

            <!-- <img src="./static/images/logo_4.png" class="header-image" style="max-width:4cm; height: auto; vertical-align: middle; margin-right: 10px;"> -->
            <h1 id="Authorship-Attribution-in-the-Era-of-LLMs" class="title is-1 publication-title">
              <img src="./static/images/icon.svg" alt="logo" width="42" height="42">
              Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges
            </h1>
            <h1 id="Authorship-Attribution-in-the-Era-of-LLMs" class="is-size-5 publication-title has-text-justified">
              TLDR: This survey paper systematically categorizes authorship attribution in the era of LLMs into four problems: attributing unknown texts to human authors, detecting LLM-generated texts, identifying specific LLMs or human authors, and classifying texts as human-authored, machine-generated, or co-authored by both, while also highlighting key challenges and open problems.
            </h1>
              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://baixianghuang.github.io/" target="_blank">Baixiang Huang</a>,</span>
                <span class="author-block">
                  <a href="https://canyuchen.com" target="_blank">Canyu Chen</a>,</span>
                <span class="author-block">
                  <a href="https://www.cs.emory.edu/~kshu5/" target="_blank">Kai Shu</a></span>
              </div>
              <div class="is-size-5 publication-institutions">
                <span class="institution-block">Emory University</span>
              </div>          
              <!-- <br> -->
              <div class="column has-text-centered">
                            <!-- Publication links -->
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2408.08946.pdf" target="_blank" class="external-link button">                 
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2408.08946" target="_blank" class="external-link button">                 
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Arxiv</span>
                    </a>
                  </span>
                
                  <span class="link-block">
                    <a href="https://github.com/llm-authorship/survey" target="_blank" class="external-link button">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Paper List</span>
                      </a>
                  </span>
                  <br>
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/" target="_blank" class="external-link button">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://drive.google.com/file/d/1pUkoDYDxeWl4nhCy74jaDIhxBW7_Sy4g/view?usp=sharing" target="_blank" class="external-link button">                 
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://x.com/"
                      class="external-link button">
                      <span class="icon">
                          <i class="fab fa-twitter"></i>
                      </span>
                      <span>post</span>
                      </a>
                  </span>
                  <span class="link-block">
                    <a href="https://www.linkedin.com/posts/"
                      class="external-link button">
                      <span class="icon">
                          <i class="fab fa-linkedin"></i>
                      </span>
                      <span>post</span>
                      </a>
                  </span> -->
                </div>
              </div>
              <div class="is-size-5 publication-authors">
                <b><i>ACM SIGKDD Explorations 2024</i></b>
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop" style="text-align: left;">
      <figure style="display: inline-block;">
        <img src="static/images/framework.png" alt="framework" style="max-width: 100%; height: auto;">
        <figcaption>
          Representative Problems in Authorship Attribution: 
          <ol>
              <li>Human-written Text Attribution (attributing unknown texts to human authors)</li>
              <li>LLM-generated Text Detection (detecting if texts are generated by LLMs)</li>
              <li>LLM-generated Text Attribution (identifying the specific LLM or human responsible for a text)</li>
              <li>Human-LLM Co-authored Text Attribution (classifying texts as human-written, machine-generated, or a combination of both)</li>
          </ol>
          <!-- <em>The categorization of these problems becomes increasingly complex, as indicated by the arrow, balancing complexity with practicality.</em> -->
      </figcaption>
      </figure>
    </div>
    <br><br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Accurate attribution of authorship is crucial for maintaining the integrity of digital content, improving forensic investigations, and mitigating the risks of misinformation and plagiarism. Addressing the imperative need for proper authorship attribution is essential to uphold the credibility and accountability of authentic authorship. The rapid advancements of Large Language Models (LLMs) have blurred the lines between human and machine authorship, posing significant challenges for traditional methods. We present a comprehensive literature review that examines the latest research on authorship attribution in the era of LLMs. This survey systematically explores the landscape of this field by categorizing four representative problems: (1) Human-written Text Attribution; (2) LLM-generated Text Detection; (3) LLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution. We also discuss the challenges related to ensuring the generalization and explainability of authorship attribution methods. Generalization requires the ability to generalize across various domains, while explainability emphasizes providing transparent and understandable insights into the decisions made by these models. By evaluating the strengths and limitations of existing methods and benchmarks, we identify key open problems and future research directions in this field. This literature review serves a roadmap for researchers and practitioners interested in understanding the state of the art in this rapidly evolving field.
              <!-- </span> -->
            </p>                   
          </div>
        </div>
      </div>
    </div>
    
    <!-- <div class="container is-max-desktop">
      <div style="text-align:left">
        <h2 class="title is-3">Contributions</h2>
      </div>
      <div class="content has-text-justified">
        Some contributions
      </div>
    </div> -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Contributions</h2>
  
          <div class="content has-text-justified">
            <!-- <p>
              There's a lot of excellent work that was introduced around the same time as ours.
            </p> -->
            <br>
              <li>We provide a timely overview to discuss the challenges and opportunities presented by LLMs in the field of authorship attribution. By systematically categorizing authorship attribution into four problems and balancing problem complexity with practicality, we reveal insights into the evolving filed of authorship attribution in the era of LLMs.</li>
              <li>We offer a comprehensive comparison of state-of-the-art methodologies, datasets, benchmarks, and commercial tools used in authorship attribution. This analysis not only improves the understanding of authorship attribution but also provides a valuable resource for researchers and practitioners to use as guidelines for approaching this direction.</li>
              <li>We discuss open issues and provide future directions by considering crucial aspects such as generalization, explainability, and interdisciplinary perspectives. We also discuss the broader implications of authorship attribution in real-world applications. This holistic approach ensures that authorship attribution not only yields accurate results but also provides insights that are explainable and socially relevant.</li>
            <br/>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->
  
    </div>
  </section>
  


<section class="section">
  <div class="container is-max-desktop content">
    
    <h2 id="Benchmarks" class="title is-3">Benchmarks</h2>
    <p>The table below is a summary of Authorship Attribution Datasets and Benchmarks with LLM-Generated Text. Size is shown as the sum of LLM-generated and human-written texts (with the percentage of human-written texts in parentheses). Language is displayed using the two-letter ISO 639 abbreviation. Columns P2, P3, and P4 indicate whether the dataset supports problems described in Problem 2, 3, and 4, respectively.</p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Domain</th>
          <th>Size</th>
          <th>Length</th>
          <th>Language</th>
          <th>Model</th>
          <th>P2</th>
          <th>P3</th>
          <th>P4</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><a href="https://arxiv.org/pdf/2109.13296">TuringBench</a></td>
          <td>News</td>
          <td>168,612 (5.2%)</td>
          <td>100 to 400 words</td>
          <td>en</td>
          <td>GPT-1,2,3, GROVER, CTRL, XLM, XLNET, FAIR, TRANSFORMER-XL, PPLM</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0251415">TweepFake</a></td>
          <td>Social media</td>
          <td>25,572 (50.0%)</td>
          <td>less than 280 characters</td>
          <td>en</td>
          <td>GPT-2, RNN, Markov, LSTM, CharRNN</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2304.07666">ArguGPT</a></td>
          <td>Academic essays</td>
          <td>8,153 (49.5%)</td>
          <td>300 words on average</td>
          <td>en</td>
          <td>GPT2-Xl, text-babbage-001, text-curie-001, davinci-001,002,003, GPT-3.5-Turbo</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2309.11285">AuTexTification</a></td>
          <td>Tweets, reviews, news, legal, and how-to articles</td>
          <td>163,306 (42.5%)</td>
          <td>20 to 100 tokens</td>
          <td>en, es</td>
          <td>BLOOM, GPT-3</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2304.12008">CHEAT</a></td>
          <td>Academic paper abstracts</td>
          <td>50,699 (30.4%)</td>
          <td>163.9 words on average</td>
          <td>en</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2306.05524">GPABench2</a></td>
          <td>Academic paper abstracts</td>
          <td>2.385M (6.3%)</td>
          <td>70 to 350 words</td>
          <td>en</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td>✓</td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2305.15047">Ghostbuster</a></td>
          <td>News, student essays, creative writing</td>
          <td>23,091 (87.0%)</td>
          <td>77 to 559 (median words per document)</td>
          <td>en</td>
          <td>ChatGPT, Claude</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2301.07597">HC3</a></td>
          <td>Reddit, Wikipedia, medicine, finance</td>
          <td>125,230 (64.5%)</td>
          <td>25 to 254 words</td>
          <td>en, zh</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2309.02731">HC3 Plus</a></td>
          <td>News, social media</td>
          <td>214,498</td>
          <td>N/A</td>
          <td>en, zh</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2310.01307">HC-Var</a></td>
          <td>News, reviews, essays, QA</td>
          <td>144k (68.8%)</td>
          <td>50 to 200 words</td>
          <td>en</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2310.16746">HANSEN</a></td>
          <td>Transcripts of speech (spoken text), statements (written text)</td>
          <td>535k (96.1%)</td>
          <td>less than 1k tokens</td>
          <td>en</td>
          <td>ChatGPT, PaLM2, Vicuna-13B</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2305.14902">M4</a></td>
          <td>Wikipedia, WikiHow, Reddit, QA, news, paper abstracts, peer reviews</td>
          <td>147,895 (24.2%)</td>
          <td>more than 1k characters</td>
          <td>ar, bg, en, id, ru, ur, zh</td>
          <td>davinci-003, ChatGPT, GPT-4, Cohere, Dolly2, BLOOMz</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2303.14822">MGTBench</a></td>
          <td>News, student essays, creative writing</td>
          <td>21k (14.3%)</td>
          <td>1 to 500 words</td>
          <td>en</td>
          <td>ChatGPT, ChatGLM, Dolly, GPT4All, StableLM, Claude</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2310.13606">MULTITuDE</a></td>
          <td>News</td>
          <td>74,081 (10.8%)</td>
          <td>200 to 512 tokens</td>
          <td>ar, ca, cs, de, en, es, nl, pt, ru, uk, zh</td>
          <td>GPT-3,4, ChatGPT, Llama-65B, Alpaca-LoRa-30B, Vicuna-13B, OPT-66B, OPT-IML-Max-1.3B</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/pdf/2305.07969">OpenGPTText</a></td>
          <td>OpenWebText</td>
          <td>58,790 (50.0%)</td>
          <td>less than 2k words</td>
          <td>en</td>
          <td>ChatGPT</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2311.08723">OpenLLMText</a></td>
          <td>OpenWebText</td>
          <td>344,530 (20%)</td>
          <td>512 tokens</td>
          <td>en</td>
          <td>ChatGPT, PaLM, Llama, GPT2-XL</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://aclanthology.org/2023.trustnlp-1.17/">Scientic Paper</a></td>
          <td>Scientific papers</td>
          <td>29k (55.2%)</td>
          <td>900 tokens on average</td>
          <td>en</td>
          <td>SCIgen, GPT-2,3, ChatGPT, Galactica</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2405.07940">RAID</a></td>
          <td>News, Wikipedia, paper abstracts, recipes, Reddit, poems, book summaries, movie reviews</td>
          <td>523,985 (2.9%)</td>
          <td>323 tokens on average</td>
          <td>cs, de, en</td>
          <td>GPT-2,3,4, ChatGPT, Mistral-7B, MPT-30B, Llama2-70B, Cohere command and chat</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2402.11175">M4GT-Bench</a></td>
          <td>Wikipedia, Wikihow, Reddit, arXiv abstracts, academic paper reviews, student essays</td>
          <td>5,368,998 (96.6%)</td>
          <td>more than 50 characters</td>
          <td>ar, bg, de, en, id, it, ru, ur, zh</td>
          <td>ChatGPT, davinci-003, GPT-4, Cohere, Dolly-v2, BLOOMz</td>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
        </tr>
        <tr>
          <td><a href="https://arxiv.org/abs/2305.13242">MAGE</a></td>
          <td>Reddit, reviews, news, QA, story writing, Wikipedia, academic paper abstracts</td>
          <td>448,459 (34.4%)</td>
          <td>263 words on average</td>
          <td>en</td>
          <td>GPT, Llama, GLM-130B, FLAN-T5 OPT, T0, BLOOM-7B1, GPT-J-6B, GPT-NeoX-2</td>
          <td>✓</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td><a href="https://aclanthology.org/2024.findings-naacl.29/">MIXSET</a></td>
          <td>Email, news, game reviews, academic paper abstracts, speeches, blogs</td>
          <td>3.6k (16.7%)</td>
          <td>50 to 250 words</td>
          <td>en</td>
          <td>GPT-4, Llama2</td>
          <td>✓</td>
          <td></td>
          <td>✓</td>
        </tr>
      </tbody>
    </table>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 id="Detectors" class="title">Detectors</h2>
    <p>The Table below presents an overview of LLM-Generated Text Detectors.</p>
    <table>
      <thead>
        <tr>
          <th>Detector</th>
          <th>Price</th>
          <th>API</th>
          <th>Website</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>GPTZero</td>
          <td>150k words at $10/month, 10k words for free per month</td>
          <td>Yes</td>
          <td><a href="https://gptzero.me/">https://gptzero.me/</a></td>
        </tr>
        <tr>
          <td>ZeroGPT</td>
          <td>100k characters for $9.99, 15k characters for free</td>
          <td>Yes</td>
          <td><a href="https://www.zerogpt.com/">https://www.zerogpt.com/</a></td>
        </tr>
        <tr>
          <td>Sapling</td>
          <td>50k characters for $25, 2k characters for free</td>
          <td>Yes</td>
          <td><a href="https://sapling.ai/ai-content-detector">https://sapling.ai/ai-content-detector</a></td>
        </tr>
        <tr>
          <td>Originality.AI</td>
          <td>200k words at $14.95/month</td>
          <td>Yes</td>
          <td><a href="https://originality.ai/">https://originality.ai/</a></td>
        </tr>
        <tr>
          <td>CopyLeaks</td>
          <td>300k words at $7.99/month</td>
          <td>Yes</td>
          <td><a href="https://copyleaks.com/ai-content-detector">https://copyleaks.com/ai-content-detector</a></td>
        </tr>
        <tr>
          <td>Winston</td>
          <td>80k words at $12/month</td>
          <td>Yes</td>
          <td><a href="https://gowinston.ai/">https://gowinston.ai/</a></td>
        </tr>
        <tr>
          <td>GPT Radar</td>
          <td>$0.02/100 tokens</td>
          <td>N/A</td>
          <td><a href="https://gptradar.com/">https://gptradar.com/</a></td>
        </tr>
        <tr>
          <td>Turnitin’s AI detector</td>
          <td>License required</td>
          <td>N/A</td>
          <td><a href="https://www.turnitin.com/solutions/topics/ai-writing/ai-detector/">https://www.turnitin.com/solutions/topics/ai-writing/ai-detector/</a></td>
        </tr>
        <tr>
          <td>GPT-2 Output Detector</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://github.com/openai/gpt-2-output-dataset/tree/master/detector">https://github.com/openai/gpt-2-output-dataset/tree/master/detector</a></td>
        </tr>
        <tr>
          <td>Crossplag</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://crossplag.com/ai-content-detector/">https://crossplag.com/ai-content-detector/</a></td>
        </tr>
        <tr>
          <td>CatchGPT</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://www.catchgpt.ai/">https://www.catchgpt.ai/</a></td>
        </tr>
        <tr>
          <td>Quil.org</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://aiwritingcheck.org/">https://aiwritingcheck.org/</a></td>
        </tr>
        <tr>
          <td>Scribbr</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://www.scribbr.com/ai-detector/">https://www.scribbr.com/ai-detector/</a></td>
        </tr>
        <tr>
          <td>Draft Goal</td>
          <td>Free</td>
          <td>N/A</td>
          <td><a href="https://detector.dng.ai/">https://detector.dng.ai/</a></td>
        </tr>
        <tr>
          <td>Writefull</td>
          <td>Free</td>
          <td>Yes</td>
          <td><a href="https://x.writefull.com/gpt-detector">https://x.writefull.com/gpt-detector</a></td>
        </tr>
        <tr>
          <td>Phrasly</td>
          <td>Free</td>
          <td>Yes</td>
          <td><a href="https://phrasly.ai/ai-detector">https://phrasly.ai/ai-detector</a></td>
        </tr>
        <tr>
          <td>Writer</td>
          <td>Free</td>
          <td>Yes</td>
          <td><a href="https://writer.com/ai-content-detector">https://writer.com/ai-content-detector</a></td>
        </tr>
      </tbody>
    </table>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 id="BibTeX" class="title">BibTeX</h2>
<pre><code>@article{huang2024aa_llm,
    title   = {Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges},
    author  = {Baixiang Huang and Canyu Chen and Kai Shu},
    year    = {2024},
    journal = {arXiv preprint arXiv: 2408.08946},
    url     = {https://arxiv.org/abs/2408.08946}, 
}</code></pre>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <!-- <div class="content has-text-justified">
              <a href="https://arxiv.org/pdf/2403.08213/" target="_blank"> (New Preprint) <b>Can Large Language Models Identify Authorship?</b></a>
              </div> -->
            </div>
          </div>

          <!-- <img src="./static/images/logo_4.png" class="header-image" style="max-width:4cm; height: auto; vertical-align: middle; margin-right: 10px;"> -->
          <h1 id="canllm-identify-authorship" class="title is-2 publication-title">Can Large Language Models Identify Authorship?</h1>
          <!-- <h1 id="canllm-identify-authorship" class="is-size-5 publication-title">TLDR: </h1> -->
            <!-- <br> -->
            <h1 id="Authorship-Attribution-in-the-Era-of-LLMs" class="is-size-5 publication-title has-text-justified">
              TLDR: We discover LLMs' strong capacities of performing <i>authorship verification</i> and <i>attribution</i> in a zero-shot way, which have surpassed the state-of-the-art supervised models, and providing <i>explanations</i> through the role of linguistic features.
            </h1>
              <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://baixianghuang.github.io/" target="_blank">Baixiang Huang</a>,</span>
              <span class="author-block">
                <a href="https://canyuchen.com" target="_blank">Canyu Chen</a>,</span>
              <span class="author-block">
                <a href="https://www.cs.emory.edu/~kshu5/" target="_blank">Kai Shu</a></span>
            </div>
            <div class="is-size-5 publication-institutions">
              <span class="institution-block">Emory University</span>
            </div>          
            <!-- <br> -->
          <!-- Publication links -->
          <div class="column has-text-centered">
          
          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2403.08213" target="_blank" class="external-link button">                 
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <!-- <span class="link-block">
              <a href="https://arxiv.org/abs/" target="_blank" class="external-link button">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span> -->
            <span class="link-block">
              <a href="https://github.com/baixianghuang/authorship-llm" target="_blank" class="external-link button">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code & Data</span>
                </a>
            </span>
            <br>
          </div>
        </div>
        <div class="is-size-5 publication-authors">
          <b><i>EMNLP 2024 Findings</i></b>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop" style="text-align: justify;">
    <figure style="display: inline-block;">
      <img src="static/images/case.png" alt="framework" style="max-width: 100%; height: auto;">
      <figcaption>
      <!-- An illustration of Authorship Analysis through Linguistically Informed Prompting (LIP) technique on the Blog Dataset: The LLM correctly identifies that the two input texts are written by the same author and provides explanations. Linguistic features detected by the model are highlighted in different colors. -->
      A Comparison Between Linguistically Informed Prompting (LIP) and other Prompting Strategies for Authorship Verification. "Analysis" and "Answer" are the output of prompting GPT-4. Only LIP strategy correctly identifies that the given two texts belong to the same author. Text colored in orange highlights the differences compared to vanilla prompting with no guidance. Text colored in blue indicates the linguistically informed reasoning process. Blue text represents the text referenced from the original documents.
    </figcaption>
    </figure>
  </div>
  <br><br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing insights into their decision-making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis.
          </p>                   
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Contributions</h2>

        <div class="content has-text-justified">
          <!-- <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p> -->
          <br>
            <li>We conduct a comprehensive evaluation of LLMs in authorship attribution and verification tasks. Our results demonstrate that LLMs outperform existing BERT-based models in a zero-shot setting, showcasing their inherent stylometric knowledge essential for distinguishing authorship. This enables them to excel in authorship attribution and verification across low-resource domains without the need for domain-specific fine-tuning.</li>
            <li>We develop a pipeline for authorship analysis with LLMs, encompassing dataset preparation, baseline implementation, and evaluation. Our novel Linguistically Informed Prompting (LIP) technique guides LLMs to leverage linguistic features for accurate authorship analysis, enhancing their reasoning capabilities.</li>
            <li>Our end-to-end approach improves the explainability of authorship analysis. It elucidates the reasoning and evidence behind authorship predictions, shedding light on how various linguistic features influence these predictions. This contributes to a deeper understanding of the mechanisms behind LLM-based authorship identification.</li>
          <br/>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>@artile{huang2024authorship,
    title   = {Can Large Language Models Identify Authorship?}, 
    author  = {Baixiang Huang and Canyu Chen and Kai Shu},
    year    = {2024},
    journal = {arXiv preprint},
    volume  = {abs/2403.08213},
    url     = {https://arxiv.org/abs/2403.08213}, 
}</code></pre>
</div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <!-- <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p> -->
            <p id="Contact">Contact: <a href="mailto:bhuang15@hawk.iit.edu">Baixiang Huang</a></p>
            <p>
              Acknowledgment: This website use code from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
              <!-- we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website. -->
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


<!-- <script>
  let slideIndexes = { 'carousel1': 1, 'carousel2': 1 };
  
  function moveSlide(n, carouselId) {
    let slides = document.querySelector('#' + carouselId + ' .carousel-inner').getElementsByClassName("carousel-item");
    slideIndexes[carouselId] += n;
    if (slideIndexes[carouselId] > slides.length) {slideIndexes[carouselId] = 1}
    if (slideIndexes[carouselId] < 1) {slideIndexes[carouselId] = slides.length}
    for (let i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
    }
    slides[slideIndexes[carouselId] - 1].style.display = "block";
  }
  
  // Initial display
  document.addEventListener('DOMContentLoaded', function() {
    moveSlide(0, 'carousel1');
    moveSlide(0, 'carousel2');
  });
</script> -->

  <!-- Default Statcounter code for llm-authorship.github.io
  https://llm-authorship.github.io/ -->
  <!-- <script type="text/javascript">
    var sc_project=13027110; 
    var sc_invisible=1; 
    var sc_security="8d52e5f6"; 
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
    Statcounter" href="https://statcounter.com/" target="_blank"><img
    class="statcounter" src="https://c.statcounter.com/13027110/0/8d52e5f6/1/"
    alt="Web Analytics Made Easy - Statcounter"
    referrerPolicy="no-referrer-when-downgrade"></a></div></noscript> -->
    <!-- End of Statcounter Code -->

</body>

<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6", "example_7", "example_8", "example_9", "example_10", "example_11", "example_12", "example_13", "example_14"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>

</html>